{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#Description: \n",
    "#   Processes a flat directory of WSI feature files (ResNet50)\n",
    "#   to detect distinct tissue sections using DBSCAN clustering.\n",
    "#   Filters spatial noise using local neighbor density.\n",
    "#   Outputs a CSV and plot for each WSI with section IDs.\n",
    "#\n",
    "# Input:\n",
    "#   - FEATURE_DIR: Directory containing .pickle files with patch features\n",
    "#\n",
    "# Output:\n",
    "#   - CSV_DIR: CSV files listing patch coordinates and section IDs\n",
    "#   - PLOT_DIR: PNG plots of clustered tissue sections\n",
    "\n",
    "# === IMPORTS ===\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Safe for multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# === CONFIG ===\n",
    "FEATURE_DIR = \"data/features/features_from_resnet50\"\n",
    "CSV_DIR     = \"data/section_assignment_csvs\"\n",
    "PLOT_DIR    = \"data/section_assignment_plots\"\n",
    "\n",
    "# clustering/filter params\n",
    "EPS          = 10\n",
    "MIN_SAMPLES  = 50\n",
    "NOISE_RADIUS = 400\n",
    "MIN_NEIGHBORS= 3\n",
    "\n",
    "# ensure output dirs exist\n",
    "os.makedirs(CSV_DIR,  exist_ok=True)\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# === helpers ===\n",
    "def extract_coords(patch_name):\n",
    "    parts = patch_name.replace(\".png\", \"\").split(\"_\")\n",
    "    if 'tile' in parts:\n",
    "        try:\n",
    "            idx = parts.index('tile')\n",
    "            return [int(parts[idx+1]), int(parts[idx+2])]\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def filter_noise(coords, radius=NOISE_RADIUS, min_neighbors=MIN_NEIGHBORS):\n",
    "    nbrs   = NearestNeighbors(radius=radius).fit(coords)\n",
    "    counts = np.array([len(nbrs.radius_neighbors([pt], return_distance=False)[0]) \n",
    "                       for pt in coords])\n",
    "    mask   = counts >= min_neighbors\n",
    "    return coords[mask], mask\n",
    "\n",
    "# === slide‚Äêlevel processing ===\n",
    "def process_slide(file):\n",
    "    slide_id    = file.replace(\"_resnet50Features_dict.pickle\", \"\")\n",
    "    pickle_path = os.path.join(FEATURE_DIR, file)\n",
    "    csv_out     = os.path.join(CSV_DIR,    f\"{slide_id}_sections.csv\")\n",
    "    plot_out    = os.path.join(PLOT_DIR,   f\"{slide_id}_sections.png\")\n",
    "\n",
    "    try:\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            patch_dict = pickle.load(f)\n",
    "\n",
    "        coords, names = [], []\n",
    "        for pname in patch_dict:\n",
    "            c = extract_coords(pname)\n",
    "            if c is not None:\n",
    "                coords.append(c)\n",
    "                names.append(pname)\n",
    "\n",
    "        if not coords:\n",
    "            print(f\"No valid coords in {slide_id}\")\n",
    "            return\n",
    "\n",
    "        coords_arr, mask = filter_noise(np.array(coords))\n",
    "        names_filt       = np.array(names)[mask]\n",
    "\n",
    "        if len(coords_arr) < MIN_SAMPLES:\n",
    "            print(f\"Too few valid patches in {slide_id}\")\n",
    "            return\n",
    "\n",
    "        db      = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit(coords_arr)\n",
    "        labels  = db.labels_\n",
    "        n_sect  = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "        # save CSV\n",
    "        df = pd.DataFrame({\n",
    "            \"patch_name\": names_filt,\n",
    "            \"row\":        coords_arr[:,0],\n",
    "            \"col\":        coords_arr[:,1],\n",
    "            \"section_id\": labels\n",
    "        })\n",
    "        df.to_csv(csv_out, index=False)\n",
    "\n",
    "        # save plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.scatter(coords_arr[:,1], coords_arr[:,0], c=labels, cmap='tab10', s=10)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(f\"{slide_id} | Sections: {n_sect} | EPS={EPS}, MIN={MIN_SAMPLES}\", fontsize=30)\n",
    "        plt.xlabel(\"Column\", fontsize=29)\n",
    "        plt.ylabel(\"Row\",    fontsize=29)\n",
    "        plt.xticks(fontsize=24)\n",
    "        plt.yticks(fontsize=24)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_out, dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"{slide_id} | Sections: {n_sect}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {slide_id}: {e}\")\n",
    "\n",
    "# === gather and run in parallel ===\n",
    "if __name__ == \"__main__\":\n",
    "    all_files = [f for f in os.listdir(FEATURE_DIR) if f.endswith(\".pickle\")]\n",
    "    print(f\"Starting parallel section detection on {len(all_files)} slides using {cpu_count()} cores...\")\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        pool.map(process_slide, all_files)\n",
    "    print(\"\\n All tissue sections processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Section-Wise Patch Saving for a Single Class ===\n",
    "# For each slide:\n",
    "# 1. Load full patch features (ResNet50 embeddings).\n",
    "# 2. Load corresponding section IDs from CSV (via DBSCAN clustering).\n",
    "# 3. Split patches by section and save each section as a new .pickle file.\n",
    "# 4. Append metadata to a summary CSV.\n",
    "# ===========================================================================\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# === Config ===\n",
    "ORIG_PICKLE_DIR = \"/data/features/features_from_resnet50/...\"\n",
    "SECTION_CSV_DIR = \"/data/features/section_assignment_csvs/...\"\n",
    "OUTPUT_PICKLE_DIR = \"data/features/section_patches/...\"\n",
    "SUMMARY_CSV = \"/data/features/section_patch_summary.csv\"\n",
    "\n",
    "os.makedirs(OUTPUT_PICKLE_DIR, exist_ok=True)\n",
    "\n",
    "summary = []\n",
    "\n",
    "# === Main Loop ===\n",
    "for file in os.listdir(ORIG_PICKLE_DIR):\n",
    "    if not file.endswith(\".pickle\"):\n",
    "        continue\n",
    "\n",
    "    # derive slide ID and paths\n",
    "    slide_id = file.replace(\"_resnet50Features_dict.pickle\", \"\")\n",
    "    pickle_path = os.path.join(ORIG_PICKLE_DIR, file)\n",
    "    section_csv_path = os.path.join(SECTION_CSV_DIR, f\"{slide_id}_sections.csv\")\n",
    "\n",
    "    if not os.path.exists(section_csv_path):\n",
    "        print(f\"‚ö†Ô∏è Missing section CSV for {slide_id}\")\n",
    "        continue\n",
    "\n",
    "    # Load full patch dict\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "        patch_dict = pickle.load(f)\n",
    "\n",
    "    # Load section mapping\n",
    "    df_sec = pd.read_csv(section_csv_path)\n",
    "\n",
    "    # Group patches by section\n",
    "    for section_id, group in df_sec.groupby(\"section_id\"):\n",
    "        section_dict = {\n",
    "            row[\"patch_name\"]: patch_dict[row[\"patch_name\"]]\n",
    "            for _, row in group.iterrows()\n",
    "            if row[\"patch_name\"] in patch_dict\n",
    "        }\n",
    "\n",
    "        new_slide_id = f\"{slide_id}_section{section_id}\"\n",
    "        output_path = os.path.join(OUTPUT_PICKLE_DIR, f\"{new_slide_id}.pickle\")\n",
    "\n",
    "        # Save section‚Äêlevel pickle\n",
    "        with open(output_path, \"wb\") as f_out:\n",
    "            pickle.dump(section_dict, f_out)\n",
    "\n",
    "        summary.append({\n",
    "            \"new_slide_id\": new_slide_id,\n",
    "            \"original_slide_id\": slide_id,\n",
    "            \"section_id\": section_id,\n",
    "            \"class_label\": \"normal\",\n",
    "            \"num_patches\": len(section_dict)\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ {slide_id} ‚Üí {df_sec['section_id'].nunique()} sections saved\")\n",
    "\n",
    "# Save summary CSV\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "print(f\"\\nüìÑ Summary saved to: {SUMMARY_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "INPUT_CSV = \"/data/features/section_patch_summary.csv\"\n",
    "SUMMARY_CSV = \"/data/features/expanded_wsi_summary.csv\"\n",
    "OUTPUT_PLOT = \"/data/features/expanded_wsi_distribution.png\"\n",
    "\n",
    "# --- Load section summary ---\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# --- Count WSI per class ---\n",
    "class_counts = df['class_label'].value_counts().sort_index()\n",
    "class_names = {\"2\": \"WD\", \"3\": \"MD\", \"4\": \"PD\"}\n",
    "\n",
    "# --- Save summary CSV ---\n",
    "summary_df = df.groupby('class_label')['new_slide_id'].nunique().reset_index()\n",
    "summary_df.columns = ['class_label', 'num_expanded_wsis']\n",
    "summary_df['class_name'] = summary_df['class_label'].astype(str).map(class_names)\n",
    "summary_df.to_csv(SUMMARY_CSV, index=False)\n",
    "print(f\" Saved summary: {SUMMARY_CSV}\")\n",
    "\n",
    "# --- Prepare Plot ---\n",
    "classes = [class_names[str(c)] for c in [2, 3, 4]]\n",
    "counts = [class_counts.get(c, 0) for c in [2, 3, 4]]\n",
    "x = np.arange(len(classes))\n",
    "bar_width = 0.8\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.rcParams['axes.spines.left'] = True\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "\n",
    "plt.bar(x, counts, color=[\"burlywood\", \"wheat\", \"beige\"], width=bar_width)\n",
    "plt.xticks(x, classes, fontsize=18, rotation=0)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel('Class', fontdict={'fontsize': 19, 'fontweight': 'bold', 'fontfamily': 'arial'})\n",
    "plt.ylabel('Count', fontdict={'fontsize': 19, 'fontweight': 'bold', 'fontfamily': 'arial'})\n",
    "plt.title('WSI distribution', fontdict={'fontsize': 24, 'fontweight': 'bold', 'fontfamily': 'arial'})\n",
    "plt.ylim(-10, max(counts) + 100)\n",
    "\n",
    "for i, value in enumerate(counts):\n",
    "    plt.text(x[i], value + 5, f'{value}', ha='center', va='bottom', fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PLOT, dpi=600)\n",
    "plt.close()\n",
    "print(f\" Plot saved: {OUTPUT_PLOT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
